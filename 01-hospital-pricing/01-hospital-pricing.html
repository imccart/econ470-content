<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Section 1: Hospital Pricing and Selection on Observables</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ian McCarthy | Emory University" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#E68080"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Section 1: Hospital Pricing and Selection on Observables
## <html>
<div style="float:left">

</div>
<hr color='#EB811B' size=1px width=0px>
</html>
### Ian McCarthy | Emory University
### Econ 470 &amp; HLTH 470

---


&lt;!-- Adjust some CSS code for font size and maintain R code font size --&gt;
&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 2em 1em 2em;    
}
.remark-code {
  font-size: 15px;
}
.remark-inline-code { 
    font-size: 20px;
}
&lt;/style&gt;


&lt;!-- Set R options for how code chunks are displayed and load packages --&gt;





# Table of contents

1. [Hospital Pricing](#hospital_pricing)

2. [HCRIS Data](#hcris)

3. [Causal Inference and Potential Outcomes](#causal)

4. [Average Treatment Effects](#ate)

5. [Selection Bias](#selection)

6. [Matching and Weighting](#methods)

7. [Pricing and Profit Status](#price_profit)

&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: hospital_pricing

# Background on Hospital Pricing

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;

---
# What is a hospital price?

Defining characteristic of hospital services: *it's complicated!*

--
.center[
  ![:scale 800px](pics/BillExample.jpg)
]

&lt;div class="smalltext"&gt;Brill, Steven. 2013. "Bitter Pill: Why Medical Bills are Killing Us." *Time Magazine*.&lt;/div&gt;

---
# What is a hospital price?

Lots of different payers paying lots of different prices:
- [Medicare fee-for-service prices](https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/Downloads/AcutePaymtSysfctsht.pdf)
- [Medicaid payments](https://www.kff.org/report-section/understanding-medicaid-hospital-payments-and-the-impact-of-recent-policy-changes-issue-brief/)
- Private insurance negotiations (including Medicare Advantage)
- But what about the price to patients?

--

.center[
Price `\(\neq\)` charge `\(\neq\)` cost `\(\neq\)` patient out-of-pocket spending
]

---
# What is a hospital price?

.center[
  ![:scale 600px](pics/DifferentPrices.jpg)
]

&lt;div class="smalltext"&gt;Source: &lt;a href="https://healthcarepricingproject.org/"&gt;Health Care Pricing Project&lt;/a&gt;&lt;/div&gt;


---
# What is a hospital price?
Not clear what exactly is negotiated...

--
.pull-left[
### Fee-for-service
- price per procedure
- percentage of charges
- markup over Medicare rates
]

--
.pull-right[
### Capitation
- payment per patient
- pay-for-performance
- shared savings
]

---
# Hospital prices in real life
A few empirical facts:

1. Hospital services are expensive

2. Prices vary dramatically across different areas

3. Lack of competition is a major reason for high prices

---
# Hospital prices in real life

.pull-left[
  ![:scale 450px](pics/HC_var_withinmkt_hip_ga_atlanta.png)
]

.pull-right[
  ![:scale 450px](pics/HC_var_withinmkt_kmri_ga_atlanta.png)
]

&lt;div class="smalltext"&gt;Source: &lt;a href="https://healthcarepricingproject.org/"&gt;Health Care Pricing Project&lt;/a&gt;&lt;/div&gt;



&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: hcris

# Understanding HCRIS Data

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# What is HCRIS?
Healthcare Cost Report Information System ('cost reports')
- Nursing Homes (SNFs)
- Hospice
- Home Health Agencies
- Hospitals 

---
# Hospital Cost Reports

.center[
  ![:scale 800px](pics/HCRIS.png)
]

---
# The Data

Let's work with the [HCRIS GitHub repository](https://github.com/imccart/HCRIS). But forming the dataset is up to you this time.

--
.center[
  ![:scale 700px](https://media.giphy.com/media/26DNdV3b6dqn1jzR6/giphy.gif)
]


---
# The Data



```r
hcris.data %&gt;% 
  ggplot(aes(x=as.factor(year))) + 
  geom_bar() +
  labs(
    x="Year",
    y="Number of Hospitals",
    title="Number of Hospitals per Year"
  ) + theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust=1))
```
.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/hospital-count-callout-1.png" style="display: block; margin: auto;" /&gt;
]


---
# Number of hospitals

&lt;img src="01-hospital-pricing_files/figure-html/hospital-count-output-1.png" style="display: block; margin: auto;" /&gt;

---
# Estimating hospital prices

```r
hcris.data &lt;- hcris.data %&gt;%
  mutate( discount_factor = 1-tot_discounts/tot_charges,
          price_num = (ip_charges + icu_charges + ancillary_charges)*discount_factor - tot_mcare_payment,
          price_denom = tot_discharges - mcare_discharges,
          price = price_num/price_denom)
```

---
# Estimating hospital prices

.left-code[

```r
hcris.data %&gt;% group_by(year) %&gt;% 
  filter(price_denom&gt;10, !is.na(price_denom), 
         price_num&gt;0, !is.na(price_num)) %&gt;%  
  select(price, year) %&gt;% 
  summarize(mean_price=mean(price, na.rm=TRUE)) %&gt;%
  ggplot(aes(x=as.factor(year), y=mean_price)) + 
  geom_line(aes(group=1)) +
  labs(
    x="Year",
    y="Average Hospital Price",
    title="Hospital Prices per Year"
  ) + scale_y_continuous(labels=comma) +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust=1))
```
]

.right-plot[
![](01-hospital-pricing_files/figure-html/price-plot1-1.png)
]


---
# Estimating hospital prices

.left-code[

```r
hcris.data %&gt;% group_by(year) %&gt;% 
  filter(price_denom&gt;100, !is.na(price_denom), 
         price_num&gt;0, !is.na(price_num),
*        price&lt;100000) %&gt;%
  select(price, year) %&gt;% 
  summarize(mean_price=mean(price, na.rm=TRUE)) %&gt;%
  ggplot(aes(x=as.factor(year), y=mean_price)) + 
  geom_line(aes(group=1)) +
  labs(
    x="Year",
    y="Average Hospital Price",
    title="Hospital Prices per Year"
  ) + scale_y_continuous(labels=comma) +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust=1))
```
]

.right-plot[
![](01-hospital-pricing_files/figure-html/price-plot2-1.png)
]






&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: causal

# Causal Inference and Potential Outcomes

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# Why causal inference?







&lt;img src="01-hospital-pricing_files/figure-html/dartmouth-output-1.png" style="display: block; margin: auto;" /&gt;


---
# Why causal inference?

Another example: **What price should we charge for a night in a hotel?**

--

.pull-left[
**Machine Learning**

- Focuses on prediction
- High prices are strongly correlated with higher sales
- Increase prices to attract more people?

]

.pull-right[
**Causal Inference**

- Focuses on **counterfactuals**
- What would sales look like if prices were higher?

]

---
# Goal of Causal Inference

- **Goal:** Estimate effect of some policy or program

- Key building block for causal inference is the idea of **potential outcomes**

---
# Some notation

**Treatment** `\(D_{i}\)`

`$$D_{i}=\begin{cases}
 1 \text{ with treatment} \\
 0 \text{ without treatment}
\end{cases}$$`


---
# Some notation

**Potential outcomes**

- `\(Y_{1i}\)` is the potential outcome for unit `\(i\)` with treatment
- `\(Y_{0i}\)` is the potential outcome for unit `\(i\)` without treatment

---
# Some notation

**Observed outcome**

`$$Y_{i}=Y_{1i} \times D_{i} + Y_{0i} \times (1-D_{i})$$`
or

`$$Y_{i}=\begin{cases}
Y_{1i} \text{ if } D_{i}=1 \\
Y_{0i} \text{ if } D_{i}=0
\end{cases}$$`


.footnote[
Assumes **SUTVA** (stable unit treatment value assumption)...no interference across units
]

---
# Example of "Potential Outcomes"

.pull-left[
![:scale 420px](pics/EmoryPicture.jpg)

`\(Y_{1}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;75,000

]

.pull-right[
![:scale 370px](pics/UNTPicture.jpg)

`\(Y_{0}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;60,000
]


---
# Example of "Potential Outcomes"
.pull-left[
![:scale 420px](pics/EmoryPicture.jpg)

`\(Y_{1}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;75,000

]

.pull-right[
![:scale 370px](pics/UNTPicture.jpg)

`\(Y_{0}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;60,000
]


Earnings due to Emory = `\(Y_{1}-Y_{0}\)` = &lt;span&gt;&amp;#36;&lt;/span&gt;15,000

---
# Example of "Potential Outcomes"

.pull-left[
![:scale 420px](pics/EmoryPicture.jpg)

`\(Y_{1}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;75,000

]

.pull-right[
![:scale 370px](pics/UNTPicture.jpg)

`\(Y_{0}\)`= ?
]


---
# Example of "Potential Outcomes"

.pull-left[
![:scale 420px](pics/EmoryPicture.jpg)

`\(Y_{1}\)`= &lt;span&gt;&amp;#36;&lt;/span&gt;75,000

]

.pull-right[
![:scale 370px](pics/UNTPicture.jpg)

`\(Y_{0}\)`= ?
]


Earnings due to Emory = `\(Y_{1}-Y_{0}\)` = ?

---
# Do we ever observe the potential outcomes?

.center[
  ![:scale 700px](https://media.giphy.com/media/zZeCRfPyXi9UI/giphy.gif)
]

--
Without a time machine...not possible to get *individual* effects.

---
# Fundamental Problem of Causal Inference

- We don't observe the counterfactual outcome...what would have happened if a treated unit was actually untreated.

- *ALL* attempts at causal inference represent some attempt at estimating the counterfactual outcome. We need an estimate for `\(Y_{0}\)` among those that were treated, and vice versa for `\(Y_{1}\)`.


---
class: inverse, center, middle
name: ate

# Average Treatment Effects

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# Different treatment effects

Tend to focus on **averages**&lt;sup&gt;1&lt;/sup&gt;:

- **ATE**: `\(\delta_{ATE} = E[ Y_{1} - Y_{0}]\)`

- **ATT**: `\(\delta_{ATT} = E[ Y_{1} - Y_{0} | D=1]\)`

- **ATU**: `\(\delta_{ATU} = E[ Y_{1} - Y_{0} | D=0]\)`


.footnote[&lt;sup&gt;1&lt;/sup&gt; or similar measures such as medians or quantiles]

---
# Average Treatment Effects

- **Estimand**: `$$\delta_{ATE} = E[Y_{1} - Y_{0}] = E[Y | D=1] - E[Y | D=0]$$`

- **Estimate**: `$$\hat{\delta}_{ATE} = \frac{1}{N_{1}} \sum_{D_{i}=1} Y_{i} - \frac{1}{N_{0}} \sum_{D_{i}=0} Y_{i},$$` where `\(N_{1}\)` is number of treated and `\(N_{0}\)` is number untreated (control)

- With random assignment and equal groups, inference/hypothesis testing with standard two-sample t-test


&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: selection

# Selection Bias

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# Selection bias

- Assume (for simplicity) constant effects, `\(Y_{1i}=Y_{0i} + \delta\)`

- Since we don't observe `\(Y_{0}\)` and `\(Y_{1}\)`, we have to use the observed outcomes, `\(Y_{i}\)`

`$$\begin{align}
E[Y_{i} | D_{i}=1] &amp;- E[Y_{i} | D_{i}=0] \\
=&amp; E[Y_{1i} | D_{i}=1] - E[Y_{0i} | D_{i}=0] \\
=&amp; \delta + E[Y_{0i} | D_{i}=1] - E[Y_{0i} | D_{i}=0] \\
=&amp; \text{ATE } + \text{ Selection Bias}
\end{align}$$`

---
# Selection bias

- Selection bias means `\(E[Y_{0i} | D_{i}=1] - E[Y_{0i} | D_{i}=0] \neq 0\)`

- In words, the potential outcome without treatment, `\(Y_{0i}\)`, is different between those that ultimately did and did not receive treatment.

- e.g., treated group was going to be better on average even without treatment (higher wages, healthier, etc.)

---
# Selection bias

- How to "remove" selection bias?

- How about random assignment?

- In this case, treatment assignment doesn't tell us anything about `\(Y_{0i}\)`

`$$E[Y_{0i}|D_{i}=1] = E[Y_{0i}|D_{i}=0],$$` such that `$$E[Y_{i}|D_{i}=1] - E[Y_{i} | D_{i}=0] = \delta_{ATE} = \delta_{ATT} = \delta_{ATU}$$`


---
# Selection bias

- Without random assignment, there's a high probability that `$$E[Y_{0i}|D_{i}=1] \neq E[Y_{0i}|D_{i}=0]$$`

- i.e., outcomes without treatment are different for the treated group


---
# Omitted variables bias

- In a regression setting, selection bias is the same problem as omitted variables bias (OVB)

- Quick review: Goal of OLS is to find `\(\hat{\beta}\)` to "best fit" the linear equation `\(y_{i} = \alpha + x_{i} \beta + \epsilon_{i}\)`

---
# Regression review

`$$\begin{align}
\min_{\beta} &amp; \sum_{i=1}^{N} \left(y_{i} - \alpha - x_{i} \beta\right)^{2}  = \min_{\beta} \sum_{i=1}^{N} \left(y_{i} - (\bar{y} - \bar{x}\beta) - x_{i} \beta\right)^{2}\\
0 &amp;= \sum_{i=1}^{N} \left(y_{i} - \bar{y} - (x_{i} - \bar{x})\hat{\beta} \right)(x_{i} - \bar{x}) \\
0 &amp;= \sum_{i=1}^{N} (y_{i} - \bar{y})(x_{i} - \bar{x}) - \hat{\beta} \sum_{i=1}^{N}(x_{i} - \bar{x})^{2} \\
\hat{\beta} &amp;= \frac{\sum_{i=1}^{N} (y_{i} - \bar{y})(x_{i} - \bar{x})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}} = \frac{Cov(y,x)}{Var(x)}
\end{align}$$`

---
# Omitted variables bias

- Interested in estimate of the effect of schooling on wages

`$$Y_{i} = \alpha + \beta s_{i} + \gamma A_{i} + \epsilon_{i}$$`

- But we don't observe ability, `\(A_{i}\)`, so we estimate

`$$Y_{i} = \alpha + \beta s_{i} + u_{i}$$`

- What is our estimate of `\(\beta\)` from this regression?

---
# Omitted variables bias

`$$\begin{align}
\hat{\beta} &amp;= \frac{Cov(Y_{i}, s_{i})}{Var(s_{i})} \\
 &amp;= \frac{Cov(\alpha + \beta s_{i} + \gamma A_{i} + \epsilon_{i}, s_{i})}{Var(s_{i})} \\
 &amp;= \frac{\beta Cov(s_{i}, s_{i}) + \gamma Cov(A_{i},s_{i}) + Cov(\epsilon_{i}, s_{i})}{Var(s_{i})}\\
 &amp;= \beta \frac{Var(s_{i})}{Var(s_{i})} + \gamma \frac{Cov(A_{i},s_{i})}{Var(s_{i})} + 0\\
 &amp;= \beta + \gamma \times \theta_{as}
 \end{align}$$`



---
# Removing selection bias without RCT

- The field of causal inference is all about different strategies to remove selection bias

- The first strategy (really, assumption) in this class: **selection on observables** or **conditional indpendence**

---
# Intuition

- Example: Does having health insurance, `\(D_{i}=1\)`, improve your health relative to someone without health insurance, `\(D_{i}=0\)`?

- `\(Y_{1i}\)` denotes health with insurance, and `\(Y_{0i}\)` health without insurance (these are **potential** outcomes)

- In raw data, `\([Y_{i} | D_{i}=1] &gt; E[Y_{i} | D_{i}=0]\)`, but is that causal?

---
# Intuition

Some assumptions:

- `\(Y_{0i}=\alpha + \eta_{i}\)`
- `\(Y_{1i} - Y_{0i} = \delta\)`
- There is some set of "controls", `\(x_{i}\)`, such that `\(\eta_{i} = \beta x_{i} + u_{i}\)` and `\(E[u_{i} | x_{i}]=0\)` (conditional independence assumption, or CIA)

--

`$$\begin{align}
Y_{i} &amp;= Y_{1i} \times D_{i} + Y_{0i} \times (1-D_{i}) \\
&amp;= \delta D_{i} + Y_{0i} D_{i} + Y_{0i} - Y_{0i} D_{i} \\
&amp;= \delta D_{i} + \alpha + \eta_{i} \\
&amp;= \delta D_{i} + \alpha + \beta x_{i} + u_{i}
\end{align}$$`

---
# ATEs versus regression coefficients

- Estimating the regression equation, `$$Y_{i} = \alpha + \delta D_{i} + \beta x_{i} + u_{i}$$` provides a causal estimate of the effect of `\(D_{i}\)` on `\(Y_{i}\)`

- But what does that really mean?

---
# ATEs vs regression coefficients

- *Ceteris paribus* ("with other conditions remaining the same"), a change in `\(D_{i}\)` will lead to a change in `\(Y_{i}\)` in the amount of `\(\hat{\delta}\)`

- But is *ceteris paribus* informative about policy?

---
# ATEs vs regression coefficients

- `\(Y_{1i} = Y_{0i} + \delta_{i} D_{i}\)` (allows for heterogeneous effects)

- `\(Y_{i} = \alpha + \beta D_{i} + \gamma X_{i} + \epsilon_{i}\)`, with `\(Y_{0i}, Y_{1i} \perp\!\!\!\perp D_{i} | X_{i}\)`

- Aronow and Samii, 2016, show that: `$$\hat{\beta} \rightarrow_{p} \frac{E[w_{i} \delta_{i}]}{E[w_{i}]},$$` where `\(w_{i} = (D_{i} - E[D_{i} | X_{i}])^{2}\)`


---
# ATEs vs regression coefficients

- Simplify to ATT and ATU
- `\(Y_{1i} = Y_{0i} + \delta_{ATT} D_{i} + \delta_{ATU} (1-D_{i})\)` 
- `\(Y_{i} = \alpha + \beta D_{i} + \gamma X_{i} + \epsilon_{i}\)`, with `\(Y_{0i}, Y_{1i} \perp\!\!\!\perp D_{i} | X_{i}\)`


--


`$$\begin{align}
\beta = &amp; \frac{P(D_{i}=1) \times \pi (X_{i} | D_{i}=1) \times (1- \pi (X_{i} | D_{i}=1))}{\sum_{j=0,1} P(D_{i}=j) \times \pi (X_{i} | D_{i}=j) \times (1- \pi (X_{i} | D_{i}=j))} \delta_{ATU} \\
&amp; + \frac{P(D_{i}=0) \times \pi (X_{i} | D_{i}=0) \times (1- \pi (X_{i} | D_{i}=0))}{\sum_{j=0,1} P(D_{i}=j) \times \pi (X_{i} | D_{i}=j) \times (1- \pi (X_{i} | D_{i}=j))} \delta_{ATT}
\end{align}$$`


---
# ATEs vs regression coefficients

What does this mean?

- OLS puts more weight on observations with treatment `\(D_{i}\)` "unexplained" by `\(X_{i}\)`

- "Reverse" weighting such that the proportion of treated units are used to weight the ATU while the proportion of untreated units enter the weights of the ATT

- This is *an* average effect, but probably not the average we want


&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: methods

# Matching and Weighting

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# Goal

Find covariates `\(X_{i}\)` such that the following assumptions are plausible:


1. Selection on observables: `$$Y_{0i}, Y_{1i} \perp\!\!\!\perp D_{i} | X_{i}$$`
2. Common support: `$$0 &lt; \text{Pr}(D_{i}=1|X_{i}) &lt; 1$$`


--


Then we can use `\(X_{i}\)` to group observations and use expectations for control as the predicted counterfactuals among treated, and vice versa. 


---
# Assumption 1: Selection on Observables

`\(E[Y_{1}|D,X]=E[Y_{1}|X]\)`


--


In words...nothing unobserved that determines treatment selection and affects your outcome of interest.


---
# Assumption 1: Selection on Observables

- Example of selection on observables from *Mastering Metrics*


---
# Assumption 2: Common Support
Someone of each type must be in both the treated and untreated groups

--
`$$0 &lt; \text{Pr}(D=1|X) &lt;1$$`




---
# Causal inference with observational data

With selection on observables and common support:

1. Subclassification
2. Matching estimators
3. Reweighting estimators
4. Regression estimators



---
# Subclassification

Sum the average treatment effects by group, and take a weighted average over those groups:

`$$ATE=\sum_{i=1}^{N} P(X=x_{i}) \left(E[Y | X, D=1] - E[Y | X, D=0]\right)$$`

---
# Subclassification

- Difference between treated and controls
- Weighted average by probability of given group (proportion of sample)
- What if outcome is unobserved for treatment or control group for a given subclass?


--

- This is the *curse of dimensionality*

---
# Matching: The process
1. For each observation `\(i\)`, find the `\(m\)` "nearest" neighbors, `\(J_{m}(i)\)`. 
2. Impute `\(\hat{Y}_{0i}\)` and `\(\hat{Y}_{1i}\)` for each observation:&lt;br&gt;
`$$\hat{Y}_{0i} = \begin{cases}
    Y_{i} &amp; \text{if} &amp; D_{i}=0 \\
    \frac{1}{m} \sum_{j \in J_{m}(i)} Y_{j} &amp; \text{if} &amp; D_{i}=1 
\end{cases}$$`
`$$\hat{Y}_{1i} = \begin{cases}
    Y_{i} &amp; \text{if} &amp; D_{i}=1 \\
    \frac{1}{m} \sum_{j \in J_{m}(i)} Y_{j} &amp; \text{if} &amp; D_{i}=0 
\end{cases}$$`

3. Form "matched" ATE:&lt;br&gt;
`\(\hat{\delta}^{\text{match}} = \frac{1}{N} \sum_{i=1}^{N} \left(\hat{Y}_{1i} - \hat{Y}_{0i} \right)\)`

---
# Matching: Defining "nearest"

1. Euclidean distance:&lt;br&gt;
`\(\sum_{k=1}^{K} (X_{ik} - X_{jk})^{2}\)`

2. Scaled Euclidean distance:&lt;br&gt;
`\(\sum_{k=1}^{K} \frac{1}{\sigma_{X_{k}}^{2}} (X_{ik} - X_{jk})^{2}\)`

3. Mahalanobis distance:&lt;br&gt;
`\((X_{i} - X_{j})' \Sigma_{X}^{-1} (X_{i} - X_{j})\)`

---
# Animation for matching


.center[
  ![:scale 900px](pics/match_animate.gif)
]

---
# Matching: Defining "nearest"

- But are observations really the same in each group?
- Potential for "matching discrepancies" to introduce bias in estimates


--

- "Bias correction" based on `$$\hat{\mu}(x_{i}) - \hat{\mu}(x_{j(i)})$$` (i.e., difference in fitted values from regression of `\(y\)` on `\(x\)`, with the difference between observed `\(Y_{1i}\)` and imputed `\(Y_{0i}\)`)

---
# Weighting

1. Estimate propensity score `ps &lt;- glm(D~X, family=binomial, data)`, denoted `\(\hat{\pi}(X_{i})\)`
2. Weight by inverse of propensity score&lt;br&gt;
.center[
`\(\hat{\mu}_{1} = \frac{ \sum_{i=1}^{N} \frac{Y_{i} D_{i}}{\hat{\pi}(X_{i})} }{ \sum_{i=1}^{N} \frac{D_{i}}{\hat{\pi}(X_{i})} }\)` and 
`\(\hat{\mu}_{0} = \frac{ \sum_{i=1}^{N} \frac{Y_{i} (1-D_{i})}{1-\hat{\pi}(X_{i})} }{ \sum_{i=1}^{N} \frac{1-D_{i}}{1-\hat{\pi}(X_{i})} }\)`
]
3. Form "inverse-propensity weighted" ATE:&lt;br&gt;
.center[
`\(\hat{\delta}^{IPW} = \hat{\mu}_{1} - \hat{\mu}_{0}\)`
]

---
# Regression
1. Regress `\(Y_{i}\)` on `\(X_{i}\)` among `\(D_{i}=1\)` to form `\(\hat{\mu}_{1}(X_{i})\)`
2. Regress `\(Y_{i}\)` on `\(X_{i}\)` among `\(D_{i}=0\)` to form `\(\hat{\mu}_{0}(X_{i})\)`
3. Form difference in predictions:&lt;br&gt;
.center[
`$$\hat{\delta}^{reg} = \frac{1}{N} \sum_{i=1}^{N} \left(\hat{\mu}_{1}(X_{i}) - \hat{\mu}_{0}(X_{i})\right)$$`
]

---
# Regression

Or estimate in one step, 
.center[
`$$Y_{i} = \delta D_{i} + \beta X_{i} + D_{i} \times \left(X_{i} - \bar{X}\right) \gamma + \varepsilon_{i}$$`
]

--

- Note the `\((X_{i} - \bar{X})\)`. What does this do?

---
# Animation for regression


.center[
  ![:scale 900px](pics/reg_animate.gif)
]


---
# Simulated data
Now let's do some matching, re-weighting, and regression with simulated data:

```r
n &lt;- 5000
select.dat &lt;- tibble(
  x = runif(n, 0, 1),
  z = rnorm(n, 0, 1),
  w = (x&gt;0.65),
  y = -2.5 + 4*w + 1.5*x + rnorm(n,0,1),
  w_alt = ( x + z &gt; 0.35),
  y_alt = -2.5 + 4*w_alt + 1.5*x + 2.25*z + rnorm(n,0,1)
)
```

---
# Simulation: nearest neighbor matching

```r
nn.est1 &lt;- Matching::Match(Y=select.dat$y,
                            Tr=select.dat$w,
                            X=select.dat$x,
                            M=1,
                            Weight=1,
                            estimand="ATE")
summary(nn.est1)
```

```
## 
## Estimate...  4.0175 
## AI SE......  0.52954 
## T-stat.....  7.5869 
## p.val......  3.2863e-14 
## 
## Original number of observations..............  5000 
## Original number of treated obs...............  1732 
## Matched number of observations...............  5000 
## Matched number of observations  (unweighted).  5016
```

---
# Simulation: nearest neighbor matching 

```r
nn.est2 &lt;- Matching::Match(Y=select.dat$y,
                            Tr=select.dat$w,
                            X=select.dat$x,
                            M=1,
*                           Weight=2,
                            estimand="ATE")
summary(nn.est2)
```

```
## 
## Estimate...  4.0175 
## AI SE......  0.52954 
## T-stat.....  7.5869 
## p.val......  3.2863e-14 
## 
## Original number of observations..............  5000 
## Original number of treated obs...............  1732 
## Matched number of observations...............  5000 
## Matched number of observations  (unweighted).  5016
```


---
# Simulation: regression

```r
reg1.dat &lt;- select.dat %&gt;% filter(w==1)
reg1 &lt;- lm(y ~ x, data=reg1.dat)

reg0.dat &lt;- select.dat %&gt;% filter(w==0)
reg0 &lt;- lm(y ~ x, data=reg0.dat)
pred1 &lt;- predict(reg1,new=select.dat)
pred0 &lt;- predict(reg0,new=select.dat)
mean(pred1-pred0)
```

```
## [1] 4.076999
```


---
# Violation of selection on observables
.pull-left[
&lt;u&gt;NN Matching&lt;/u&gt;

```r
nn.est3 &lt;- Matching::Match(Y=select.dat$y_alt,
                            Tr=select.dat$w_alt,
                            X=select.dat$x,
                            M=1,
                            Weight=2,
                            estimand="ATE")
summary(nn.est3)
```

```
## 
## Estimate...  7.6642 
## AI SE......  0.052903 
## T-stat.....  144.87 
## p.val......  &lt; 2.22e-16 
## 
## Original number of observations..............  5000 
## Original number of treated obs...............  2748 
## Matched number of observations...............  5000 
## Matched number of observations  (unweighted).  23014
```
]

.pull-right[
&lt;u&gt;Regression&lt;/u&gt;

```r
reg1.dat &lt;- select.dat %&gt;% filter(w_alt==1)
reg1 &lt;- lm(y_alt ~ x, data=reg1.dat)

reg0.dat &lt;- select.dat %&gt;% filter(w_alt==0)
reg0 &lt;- lm(y_alt ~ x, data=reg0.dat)
pred1_alt &lt;- predict(reg1,new=select.dat)
pred0_alt &lt;- predict(reg0,new=select.dat)
mean(pred1_alt-pred0_alt)
```

```
## [1] 7.646532
```
]


---
# What covariates to use?

- There are such things as "bad controls"
- We want to avoid control variables that are:


--

- Outcomes of the treatment
- Also endogenous (more generally)


&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: price_profit

# Pricing and Hospital Profit Status

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;


---
# Penalized hospitals

```r
final.hcris &lt;- hcris.data %&gt;% ungroup() %&gt;%
  filter(price_denom&gt;100, !is.na(price_denom), 
         price_num&gt;0, !is.na(price_num),
         price&lt;100000, 
*        beds&gt;30, year==2012) %&gt;%
  mutate( hvbp_payment = ifelse(is.na(hvbp_payment),0,hvbp_payment),
*         hrrp_payment = ifelse(is.na(hrrp_payment),0,abs(hrrp_payment)),
*   penalty = (hvbp_payment-hrrp_payment&lt;0))
```



---
# Summary stats
Always important to look at your data before doing any formal analysis. Ask yourself a few questions:
1. Are the magnitudes reasonable?

2. Are there lots of missing values?

3. Are there clear examples of misreporting?

---
# Summary stats

.pull-left[

```r
summary(hcris.data$price)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
## -123697    4783    7113     Inf   10230     Inf   63662
```

```r
plot(density(hcris.data$price, na.rm=TRUE))
```

&lt;img src="01-hospital-pricing_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
summary(final.hcris$price)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   340.8  6129.9  8705.4  9646.9 11905.4 97688.8
```

```r
plot(density(final.hcris$price))
```

&lt;img src="01-hospital-pricing_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;
]

---
# Dealing with problems
We've adopted a very brute force way to deal with outlier prices. Other approaches include:
1. Investigate very closely the hospitals with extreme values

2. Winsorize at certain thresholds (replace extreme values with pre-determined thresholds)

3. Impute prices for extreme hospitals

---
# Differences among penalized hospitals
- Mean price among penalized hospitals: 9,896.31
- Mean price among non-penalized hospitals: 9,560.41
- Mean difference: 335.9

---
# Comparison of hospitals
Are penalized hospitals sufficiently similar to non-penalized hospitals?

--
&lt;br&gt;
&lt;br&gt;
Let's look at covariate balance using a love plot, part of the `library(cobalt)` package.

---
# Love plots without adjustment




```r
love.plot(bal.tab(lp.covs,treat=lp.vars$penalty), colors="black", shapes="circle", threshold=0.1) + 
  theme_bw() + theme(legend.position="none")
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/cov-balance-callout-1.png" style="display: block; margin: auto;" /&gt;
]


---
# Love plots without adjustment

&lt;img src="01-hospital-pricing_files/figure-html/cov-balance-output-1.png" style="display: block; margin: auto;" /&gt;

---
# Using matching to improve balance
Some things to think about:
- exact versus nearest neighbor
- with or without ties (and how to break ties)
- measure of distance

---
# 1. Exact Matching

```r
m.exact &lt;- Matching::Match(Y=lp.vars$price,
                           Tr=lp.vars$penalty,
                           X=lp.covs,
                           M=1,
*                          exact=TRUE)
print(m.exact)
```

```
## [1] NA
## attr(,"class")
## [1] "Match"
```

---
# 1. Exact Matching (on a subset)

```r
lp.covs2 &lt;- lp.covs %&gt;% select(beds, mcaid_discharges)
m.exact &lt;- Matching::Match(Y=lp.vars$price,
                           Tr=lp.vars$penalty,
                           X=lp.covs2,
                           M=1,
                           exact=TRUE,
*                          estimand="ATE")
```

---
# 1. Exact Matching (on a subset)

```r
love.plot(bal.tab(m.exact, covs = lp.covs2, treat = lp.vars$penalty),  
          threshold=0.1, 
          grid=FALSE, sample.names=c("Unmatched", "Matched"),
          position="top", shapes=c("circle","triangle"),
          colors=c("black","blue")) + 
  theme_bw()
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/lp-exact-callout-1.png" style="display: block; margin: auto;" /&gt;
]


---
# 1. Exact Matching (on a subset)

&lt;img src="01-hospital-pricing_files/figure-html/lp-exact-output-1.png" style="display: block; margin: auto;" /&gt;


---
# 2. Nearest neighbor matching (inverse variance)

```r
m.nn.var &lt;- Matching::Match(Y=lp.vars$price,
                            Tr=lp.vars$penalty,
                            X=lp.covs,
*                           M=4,
                            Weight=1,
                            estimand="ATE")

v.name=data.frame(new=c("Beds","Medicaid Discharges", "Inaptient Charges",
                   "Medicare Discharges", "Medicare Payments"))
```

---
# 2. Nearest neighbor matching (inverse variance)


```r
love.plot(bal.tab(m.nn.var, covs = lp.covs, treat = lp.vars$penalty), 
          threshold=0.1, 
          var.names=v.name,
          grid=FALSE, sample.names=c("Unmatched", "Matched"),
          position="top", shapes=c("circle","triangle"),
          colors=c("black","blue")) + 
  theme_bw()
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/lp-var-callout1-1.png" style="display: block; margin: auto;" /&gt;
]


---
# 2. Nearest neighbor matching (inverse variance)

&lt;img src="01-hospital-pricing_files/figure-html/lp-var-output1-1.png" style="display: block; margin: auto;" /&gt;

---
# 2. Nearest neighbor matching (inverse variance)

```r
m.nn.var2 &lt;- Matching::Match(Y=lp.vars$price,
                             Tr=lp.vars$penalty,
                             X=lp.covs,
*                            M=1,
                             Weight=1,
                             estimand="ATE")
```

---
# 2. Nearest neighbor matching (inverse variance)


```r
love.plot(bal.tab(m.nn.var2, covs = lp.covs, treat = lp.vars$penalty), 
          threshold=0.1, 
          var.names=v.name,
          grid=FALSE, sample.names=c("Unmatched", "Matched"),
          position="top", shapes=c("circle","triangle"),
          colors=c("black","blue")) + 
  theme_bw()
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/lp-var-callout2-1.png" style="display: block; margin: auto;" /&gt;
]


---
# 2. Nearest neighbor matching (inverse variance)

&lt;img src="01-hospital-pricing_files/figure-html/lp-var-output2-1.png" style="display: block; margin: auto;" /&gt;


---
# 2. Nearest neighbor matching (Mahalanobis)

```r
m.nn.md &lt;- Matching::Match(Y=lp.vars$price,
                           Tr=lp.vars$penalty,
                           X=lp.covs,
                           M=1,
                           Weight=2,
                           estimand="ATE")                           
```

---
# 2. Nearest neighbor matching (Mahalanobis)


```r
love.plot(bal.tab(m.nn.md, covs = lp.covs, treat = lp.vars$penalty), 
          threshold=0.1, 
          var.names=v.name,
          grid=FALSE, sample.names=c("Unmatched", "Matched"),
          position="top", shapes=c("circle","triangle"),
          colors=c("black","blue")) + 
  theme_bw()
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/lp-md-callout1-1.png" style="display: block; margin: auto;" /&gt;
]


---
# 2. Nearest neighbor matching (Mahalanobis)

&lt;img src="01-hospital-pricing_files/figure-html/lp-md-output1-1.png" style="display: block; margin: auto;" /&gt;

---
# 2. Nearest neighbor matching (propensity score)

```r
logit.model &lt;- glm(penalty ~ beds + mcaid_discharges + ip_charges + mcare_discharges +
            tot_mcare_payment, family=binomial, data=lp.vars)
ps &lt;- fitted(logit.model)
m.nn.ps &lt;- Matching::Match(Y=lp.vars$price,
                           Tr=lp.vars$penalty,
                           X=ps,
                           M=1,
                           estimand="ATE")
```

---
# 2. Nearest neighbor matching (propensity score)


```r
love.plot(bal.tab(m.nn.ps, covs = lp.covs, treat = lp.vars$penalty), 
          threshold=0.1, 
          var.names=v.name,
          grid=FALSE, sample.names=c("Unmatched", "Matched"),
          position="top", shapes=c("circle","triangle"),
          colors=c("black","blue")) + 
  theme_bw()
```

.plot-callout[
&lt;img src="01-hospital-pricing_files/figure-html/lp-ps-callout-1.png" style="display: block; margin: auto;" /&gt;
]


---
# 2. Nearest neighbor matching (propensity score)

&lt;img src="01-hospital-pricing_files/figure-html/lp-ps-output-1.png" style="display: block; margin: auto;" /&gt;

---
# 3. Weighting
&lt;img src="01-hospital-pricing_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

---
# Results: Exact matching

```
## 
## Estimate...  1777.6 
## AI SE......  34.725 
## T-stat.....  51.191 
## p.val......  &lt; 2.22e-16 
## 
## Original number of observations..............  2707 
## Original number of treated obs...............  698 
## Matched number of observations...............  12 
## Matched number of observations  (unweighted).  12 
## 
## Number of obs dropped by 'exact' or 'caliper'  2695
```


---
# Results: Nearest neighbor

- Inverse variance

```
## 
## Estimate...  -526.95 
## AI SE......  223.06 
## T-stat.....  -2.3623 
## p.val......  0.01816 
## 
## Original number of observations..............  2707 
## Original number of treated obs...............  698 
## Matched number of observations...............  2707 
## Matched number of observations  (unweighted).  2711
```

---
# Results: Nearest neighbor

- Mahalanobis

```
## 
## Estimate...  -492.82 
## AI SE......  223.55 
## T-stat.....  -2.2046 
## p.val......  0.027485 
## 
## Original number of observations..............  2707 
## Original number of treated obs...............  698 
## Matched number of observations...............  2707 
## Matched number of observations  (unweighted).  2708
```

---
# Results: Nearest neighbor

- Propensity score

```
## 
## Estimate...  -201.03 
## AI SE......  275.76 
## T-stat.....  -0.72898 
## p.val......  0.46601 
## 
## Original number of observations..............  2707 
## Original number of treated obs...............  698 
## Matched number of observations...............  2707 
## Matched number of observations  (unweighted).  14795
```


---
# Results: IPW weighting


```r
lp.vars &lt;- lp.vars %&gt;%
  mutate(ipw = case_when(
    penalty==1 ~ 1/ps,
    penalty==0 ~ 1/(1-ps),
    TRUE ~ NA_real_
  ))
mean.t1 &lt;- lp.vars %&gt;% filter(penalty==1) %&gt;%
  select(price, ipw) %&gt;% summarize(mean_p=weighted.mean(price,w=ipw))
mean.t0 &lt;- lp.vars %&gt;% filter(penalty==0) %&gt;%
  select(price, ipw) %&gt;% summarize(mean_p=weighted.mean(price,w=ipw))
mean.t1$mean_p - mean.t0$mean_p
```

```
## [1] -196.8922
```

---
# Results: IPW weighting with regression


```r
ipw.reg &lt;- lm(price ~ penalty, data=lp.vars, weights=ipw)
summary(ipw.reg)
```

```
## 
## Call:
## lm(formula = price ~ penalty, data = lp.vars, weights = ipw)
## 
## Weighted Residuals:
##    Min     1Q Median     3Q    Max 
## -18691  -4802  -1422   2651  94137 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   9876.4      147.8  66.808   &lt;2e-16 ***
## penaltyTRUE   -196.9      211.2  -0.932    0.351    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7829 on 2705 degrees of freedom
## Multiple R-squared:  0.0003211,	Adjusted R-squared:  -4.85e-05 
## F-statistic: 0.8688 on 1 and 2705 DF,  p-value: 0.3514
```

---
# Results: Regression


```r
reg1.dat &lt;- lp.vars %&gt;% filter(penalty==1, complete.cases(.))
reg1 &lt;- lm(price ~ beds+ mcaid_discharges + ip_charges + mcare_discharges +
            tot_mcare_payment, data=reg1.dat)

reg0.dat &lt;- lp.vars %&gt;% filter(penalty==0, complete.cases(.))
reg0 &lt;- lm(price ~ beds + mcaid_discharges + ip_charges + mcare_discharges +
            tot_mcare_payment, data=reg0.dat)
pred1 &lt;- predict(reg1,new=lp.vars)
pred0 &lt;- predict(reg0,new=lp.vars)
mean(pred1-pred0)
```

```
## [1] -5.845761
```

---
# Results: Regression in one step


```r
reg.dat &lt;- lp.vars %&gt;% ungroup() %&gt;% filter(complete.cases(.)) %&gt;%
  mutate(beds_diff = penalty*(beds - mean(beds)),
         mcaid_diff = penalty*(mcaid_discharges - mean(mcaid_discharges)),
         ip_diff = penalty*(ip_charges - mean(ip_charges)),
         mcare_diff = penalty*(mcare_discharges - mean(mcare_discharges)),
         mpay_diff = penalty*(tot_mcare_payment - mean(tot_mcare_payment)))
reg &lt;- lm(price ~ penalty + beds + mcaid_discharges + ip_charges + mcare_discharges + tot_mcare_payment + 
            beds_diff + mcaid_diff + ip_diff + mcare_diff + mpay_diff,
          data=reg.dat)
```

---
# Results: Regression in one step


```
## 
## Call:
## lm(formula = price ~ penalty + beds + mcaid_discharges + ip_charges + 
##     mcare_discharges + tot_mcare_payment + beds_diff + mcaid_diff + 
##     ip_diff + mcare_diff + mpay_diff, data = reg.dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -38175  -2900   -597   2105  67409 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        8.466e+03  1.711e+02  49.482  &lt; 2e-16 ***
## penaltyTRUE       -5.846e+00  2.124e+02  -0.028  0.97804    
## beds               1.107e+00  1.421e+00   0.779  0.43618    
## mcaid_discharges  -4.714e-01  7.296e-02  -6.462 1.23e-10 ***
## ip_charges         6.426e-06  1.285e-06   5.002 6.04e-07 ***
## mcare_discharges  -8.122e-01  9.257e-02  -8.774  &lt; 2e-16 ***
## tot_mcare_payment  9.502e-05  6.858e-06  13.857  &lt; 2e-16 ***
## beds_diff          2.517e+00  2.986e+00   0.843  0.39931    
## mcaid_diff         1.058e-01  1.570e-01   0.674  0.50050    
## ip_diff           -4.534e-06  2.027e-06  -2.237  0.02539 *  
## mcare_diff         4.806e-01  1.809e-01   2.657  0.00793 ** 
## mpay_diff         -5.452e-05  1.321e-05  -4.128 3.78e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4728 on 2695 degrees of freedom
## Multiple R-squared:  0.2477,	Adjusted R-squared:  0.2447 
## F-statistic: 80.69 on 11 and 2695 DF,  p-value: &lt; 2.2e-16
```


---
# Summary of ATEs
1. Exact matching: 1777.63
2. NN matching, inverse variance: -526.95
3. NN matching, mahalanobis: -492.82
4. NN matching, pscore: -201.03
5. Inverse pscore weighting: -196.89
6. IPW regression: -196.89
7. Regression: -5.85
8. Regression 1-step: -5.85


&lt;!-- New Section --&gt;
---
class: inverse, center, middle
name: summary

# So what have we learned?

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=1055px&gt;&lt;/html&gt;

---
# Key assumptions for causal inference
1. Selection on observables
2. Common support

--
&lt;br&gt;
&lt;br&gt;

These become more nuanced but the intuition is the same in almost all questions of causal inference.

---
# Causal effect assuming selection on observables
If we assume selection on observables holds, then we only need to condition on the relevant covariates to identify a causal effect. But we still need to ensure common support...&lt;br&gt;

--
&lt;br&gt;
1. Matching
2. Reweighting
3. Regression

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
